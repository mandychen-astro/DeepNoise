Dataset:
    - PKS0405-123_OB1EXP1, PKS0405-123_OB1EXP2, HE0226-4110_OB1EXP1, SDSSJ1427-0121_OB1EXP1
    - Pre-processing:
        - Removed all spaxels with any number of missing values in spectrum
        - Got rid of first and last 10 spectral pixels
        - Rescaled data v1: rescaled_data = arcsinh(data), then clipped 3-sig outliers in the lower direction 
        - Rescaled data v2: rescaled_data = arcsinh(data), then clipped 3-sig outliers in both directions, 
                            then rescale_data_w_prior_bounds(data, log=False, input_min=3, input_max=10), put 3 at 0 and 10 at 1
        - Rescaled data v3: rescaled_data = data/3000, then clipped 3-sig outliers in the lower direction 
        - Rescaled data v4: rescaled_data = data**3/3000**3, then clipped 3-sig outliers in the lower direction 
        - Rescaled data v5: rescaled_data = data * 10, if data > 50; rescaled_data = data, if data <= 50. Then clipped 3-sig outliers in the lower direction
        - Rescaled data v6: same as v3, but resampled to a common wavelength grid, wave_grid = np.arange(4765, 9336, 1.25) 

Model:
    - v1--v7: Transformer autoencoder, see screenshot "model.png"

Loss:
    - v1--v7: MSE

Training:
    - 20-30 epochs
    - batch size 64
    - Adam optimizer with lr=0.001 for v1-v3, lr=0.01 for v4, lr=0.0001 for v5, learning rate decay for v6, lr=0.0001 w/o decay for v7,

Results:
    - v1: (used data v1) Works ok, but residual still worse than MUSE pipeline result, and also worse than previous exp. with data from only PKS0405-123_OB1EXP1
    - v2: (used data v2) basically the same as v1, rescaling the values to be approximately between 0 and 1 did not help
    - v3: (used data v3) results improved!  Yay!  Rescaling the data linearly to be between 0 and 1 helped.  The residual is still worse than the MUSE pipeline result tho.
    - v4: (used data v3) increased learning rate to 0.01, results are worse than v3. So, try decreasing learning rate to 0.0001 and see if that helps.
    - v5: (used data v3) decreased learning rate to 0.0001, there is less overfitting, and residual is comparable to v3.
    - v6: (used data v3) implemented learning rate scheduler, torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5); Model was still learning by end of epoch 20, so could have trained a little longer perhaps, but the results are not significantly different from v5 so far
    - v7: (used data v4) rescaled data by taking the cube of the data and got rid of lr scheduler; Suppression of lower values too aggressive, loss isnâ€™t sensitive to these values anymore; need to find a scaling scheme more in between
    - v8: (used data v5) rescaled data by *10 for values > 50 (roughly 80th percentile), and left the rest as is; loss is more sensitive to lower values again, but still not as much as v3; residual is comparable to v3 for high values, but worse for lower values
    - v9: (used data v6) used setting of model v5, lr 0.0001, no lr scheduler. 


Next steps:
    - Seems like having a linear rescaling of the data is the best approach so far
    - Increasing weights for higher values (v4 and v5) doesn't seem to help much
    - Next will try different architectures, maybe replacing the first FC layer with a convolutional layer